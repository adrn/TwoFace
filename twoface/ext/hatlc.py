#!/usr/bin/env python
# -*- coding: utf-8 -*-
# From: https://github.com/waqasbhatti/astrobase/

'''hatlc.py - Waqas Bhatti (wbhatti@astro.princeton.edu) - Jan 2016
License: MIT - see LICENSE for the full text.

This contains functions to read HAT sqlite ("sqlitecurves") and CSV light curves
generated by the new HAT data server.

The most probably useful functions in this module are:

read_csvlc(lcfile):
    This reads the HAT data server producd CSV light curve into an lcdict.

    lcfile is the HAT gzipped CSV LC (with a .hatlc.csv.gz extension)


read_and_filter_sqlitecurve(lcfile, columns=None, sqlfilters=None,
                            raiseonfail=False, forcerecompress=False):
    This reads the sqlitecurve and optionally filters it, returns an lcdict.

    Returns columns requested in columns. If None, then returns all columns
    present in the latest columnlist in the lightcurve. See COLUMNDEFS for the
    full list of HAT LC columns.

    If sqlfilters is not None, it must be a list of text sql filters that apply
    to the columns in the lightcurve.

    This returns an lcdict with an added 'lcfiltersql' key that indicates what
    the parsed SQL filter string was.

    If forcerecompress = True, will recompress the un-gzipped sqlitecurve even
    if the gzipped form exists on disk already.


Two other functions that might be useful:

normalize_lcdict(lcdict, timecol='rjd', magcols='all', mingap=4.0,
                 normto='sdssr', debugmode=False):

    This normalizes magnitude columns (specified in the magcols keyword
    argument) in an lcdict obtained from reading a HAT light curve. This
    normalization is done by finding 'timegroups' in each magnitude column,
    assuming that these belong to different 'eras' separated by a specified gap
    in the mingap keyword argument, and thus may be offset vertically from one
    another. Measurements within a timegroup are normalized to zero using the
    meidan magnitude of the timegroup. Once all timegroups have been processed
    this way, the whole time series is then re-normalized to the specified value
    in the normto keyword argument.


normalize_lcdict_byinst(lcdict, magcols='all', normto='sdssr', debugmode=False)

    This normalized magnitude columns (specified in the magcols keyword
    argument) in an lcdict obtained from reading a HAT light curve. This
    normalization is done by generating a normalization key using columns in the
    lcdict that specify various instrument properties. The normalization key is
    a combination of:

    - HAT station IDs
    - camera filters
    - observed HAT field names
    - HAT project IDs
    - camera exposure times

    with the assumption that measurements with identical normalization keys
    belong to a single 'era'. Measurements within an era are normalized to zero
    using the median magnitude of the era. Once all eras have been processed
    this way, the whole time series is then re-normalized to the specified value
    in the normto keyword argument.


There's an IPython notebook describing the use of this module and accompanying
modules from the astrobase package at:

https://github.com/waqasbhatti/astrobase/blob/master/notebooks/lightcurve-work.ipynb

'''

# put this in here because hatlc can be used as a standalone module
__version__ = '0.3.8'


#############
## LOGGING ##
#############

import logging
from datetime import datetime
from traceback import format_exc

# setup a logger
LOGGER = None
LOGMOD = __name__
DEBUG = False

def set_logger_parent(parent_name):
    globals()['LOGGER'] = logging.getLogger('%s.%s' % (parent_name, LOGMOD))

def LOGDEBUG(message):
    if LOGGER:
        LOGGER.debug(message)
    elif DEBUG:
        print('[%s - DBUG] %s' % (
            datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
            message)
        )

def LOGINFO(message):
    if LOGGER:
        LOGGER.info(message)
    else:
        print('[%s - INFO] %s' % (
            datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
            message)
        )

def LOGERROR(message):
    if LOGGER:
        LOGGER.error(message)
    else:
        print('[%s - ERR!] %s' % (
            datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
            message)
        )

def LOGWARNING(message):
    if LOGGER:
        LOGGER.warning(message)
    else:
        print('[%s - WRN!] %s' % (
            datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
            message)
        )

def LOGEXCEPTION(message):
    if LOGGER:
        LOGGER.exception(message)
    else:
        print(
            '[%s - EXC!] %s\nexception was: %s' % (
                datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
                message, format_exc()
                )
            )


####################
## SYSTEM IMPORTS ##
####################

import os.path
import os
import gzip
import shutil
import subprocess
import re
import sqlite3 as sql
import json

import numpy as np
from numpy import nan


#################
## DEFINITIONS ##
#################

# LC column definitions
# the first elem is the column description, the second is the format to use when
# writing a CSV LC column, the third is the type to use when parsing a CSV LC
# column
COLUMNDEFS = {
    # TIME
    'rjd':('time of observation in Reduced Julian date (JD = 2400000.0 + RJD)',
           '%.7f',
           float),
    'bjd':(('time of observation in Baryocentric Julian date '
            '(note: this is BJD_TDB)'),
           '%.7f',
           float),
    # FRAME METADATA
    'net':('network of telescopes observing this target',
           '%s',
           str),
    'stf':('station ID of the telescope observing this target',
           '%i',
           int),
    'cfn':('camera frame serial number',
           '%i',
           int),
    'cfs':('camera subframe id',
           '%s',
           str),
    'ccd':('camera CCD position number',
           '%i',
           int),
    'prj':('project ID of this observation',
           '%s',
           str),
    'fld':('observed field name',
           '%s',
           str),
    'frt':('image frame type (flat, object, etc.)',
           '%s',
           str),
    # FILTER CONFIG
    'flt':('filter ID from the filters table',
           '%i',
           int),
    'flv':('filter version used',
           '%i',
           int),
    # CAMERA CONFIG
    'cid':('camera ID ',
           '%i',
           int),
    'cvn':('camera version',
           '%i',
           int),
    'cbv':('camera bias-frame version',
           '%i',
           int),
    'cdv':('camera dark-frame version',
           '%i',
           int),
    'cfv':('camera flat-frame version',
           '%i',
           int),
    'exp':('exposure time for this observation in seconds',
           '%.3f',
           float),
    # TELESCOPE CONFIG
    'tid':('telescope ID',
           '%i',
           int),
    'tvn':('telescope version',
           '%i',
           int),
    'tfs':('telescope focus setting',
           '%i',
           int),
    'ttt':('telescope tube temperature [deg]',
           '%.3f',
           float),
    'tms':('telescope mount state (tracking, drizzling, etc.)',
           '%s',
           str),
    'tmi':('telescope mount ID',
           '%i',
           int),
    'tmv':('telescope mount version',
           '%i',
           int),
    'tgs':('telescope guider status (MGen)',
           '%s',
           str),
    # ENVIRONMENT
    'mph':('moon phase at this observation',
           '%.2f',
           float),
    'iha':('hour angle of object at this observation',
           '%.3f',
           float),
    'izd':('zenith distance of object at this observation',
           '%.3f',
           float),
    # APERTURE PHOTOMETRY METADATA
    'xcc':('x coordinate on CCD chip',
           '%.3f',
           float),
    'ycc':('y coordinate on CCD chip',
           '%.3f',
           float),
    'bgv':('sky background measurement around object in ADU',
           '%.3f',
           float),
    'bge':('error in sky background measurement in ADU',
           '%.3f',
           float),
    'fsv':('source extraction S parameter (the PSF spatial RMS)',
           '%.5f',
           float),
    'fdv':('source extraction D parameter (the PSF spatial ellipticity in xy)',
           '%.5f',
           float),
    'fkv':('source extraction K parameter (the PSF spatial diagonal ellipticity)',
           '%.5f',
           float),
    # APERTURE PHOTOMETRY COLUMNS (NOTE: these are per aperture)
    'aim':('aperture photometry raw instrumental magnitude in aperture %s',
            '%.5f',
            float),
    'aie':('aperture photometry raw instrumental mag error in aperture %s',
            '%.5f',
            float),
    'aiq':('aperture photometry raw instrumental mag quality flag for aperture %s',
            '%s',
            str),
    'arm':('aperture photometry fit magnitude in aperture %s',
            '%.5f',
            float),
    'aep':('aperture photometry EPD magnitude in aperture %s',
            '%.5f',
            float),
    'atf':('aperture photometry TFA magnitude in aperture %s',
            '%.5f',
            float),
    # PSF FIT PHOTOMETRY METADATA
    'psv':('PSF fit S parameter (the PSF spatial RMS)',
           '%.5f',
           float),
    'pdv':('PSF fit D parameter (the PSF spatial ellipticity in xy)',
           '%.5f',
           float),
    'pkv':('PSF fit K parameter (the PSF spatial diagonal ellipticity)',
           '%.5f',
           float),
    'ppx':('PSF fit number of pixels used for fit',
           '%i',
           int),
    'psn':('PSF fit signal-to-noise ratio',
           '%.3f',
           float),
    'pch':('PSF fit chi-squared value',
           '%.5f',
           float),
    # PSF FIT PHOTOMETRY COLUMNS
    'psim':('PSF fit instrumental raw magnitude',
            '%.5f',
            float),
    'psie':('PSF fit instrumental raw magnitude error',
            '%.5f',
            float),
    'psiq':('PSF fit instrumental raw magnitude quality flag',
            '%s',
            str),
    'psrm':('PSF fit final magnitude after mag-fit',
            '%.5f',
            float),
    'psrr':('PSF fit residual',
            '%.5f',
            float),
    'psrn':('PSF fit number of sources used',
            '%i',
            int),
    'psep':('PSF fit EPD magnitude',
            '%.5f',
            float),
    'pstf':('PSF fit TFA magnitude',
            '%.5f',
            float),
    # IMAGE SUBTRACTION PHOTOMETRY METADATA
    'xic':('x coordinate on CCD chip after image-subtraction frame warp',
           '%.3f',
           float),
    'yic':('y coordinate on CCD chip after image-subtraction frame warp',
           '%.3f',
           float),
    # IMAGE SUBTRACTION PHOTOMETRY COLUMNS
    'irm':('image subtraction fit magnitude in aperture %s',
            '%.5f',
            float),
    'ire':('image subtraction fit magnitude error in aperture %s',
            '%.5f',
            float),
    'irq':('image subtraction fit magnitude quality flag for aperture %s',
            '%s',
            str),
    'iep':('image subtraction EPD magnitude in aperture %s',
            '%.5f',
            float),
    'itf':('image subtraction TFA magnitude in aperture %s',
            '%.5f',
            float),
}


LC_MAG_COLUMNS = ('aim','arm','aep','atf',
                  'psim','psrm','psep','pstf',
                  'irm','iep','itf')

LC_ERR_COLUMNS = ('aie','psie','ire')

LC_FLAG_COLUMNS = ('aiq','psiq','irq')

# used to validate the filter string
# http://www.sqlite.org/lang_keywords.html
SQLITE_ALLOWED_WORDS = ['and','between','in',
                        'is','isnull','like','not',
                        'notnull','null','or',
                        '=','<','>','<=','>=','!=','%']


#######################
## UTILITY FUNCTIONS ##
#######################

# this is from Tornado's source (MIT License):
# http://www.tornadoweb.org/en/stable/_modules/tornado/escape.html#squeeze
def squeeze(value):
    """Replace all sequences of whitespace chars with a single space."""
    return re.sub(r"[\x00-\x20]+", " ", value).strip()



########################################
## SQLITECURVE COMPRESSIION FUNCTIONS ##
########################################

def pycompress_sqlitecurve(sqlitecurve, force=False):
    '''This just compresses the sqlitecurve. Should be independent of OS.

    '''

    outfile = '%s.gz' % sqlitecurve

    try:

        if os.path.exists(outfile) and not force:
            os.remove(sqlitecurve)
            return outfile

        else:

            with open(sqlitecurve,'rb') as infd:
                with gzip.open(outfile,'wb') as outfd:
                    shutil.copyfileobj(infd, outfd)

            if os.path.exists(outfile):
                os.remove(sqlitecurve)
                return outfile
            else:
                LOGERROR('could not compress %s' % sqlitecurve)

    except Exception as e:
        LOGEXCEPTION('could not compress %s' % sqlitecurve)



def pyuncompress_sqlitecurve(sqlitecurve, force=False):
    '''This just uncompresses the sqlitecurve. Should be independent of OS.

    '''

    outfile = sqlitecurve.replace('.gz','')

    try:

        if os.path.exists(outfile) and not force:
            return outfile

        else:

            with gzip.open(sqlitecurve,'rb') as infd:
                with open(outfile,'wb') as outfd:
                    shutil.copyfileobj(infd, outfd)

            # do not remove the intput file yet
            if os.path.exists(outfile):
                return outfile
            else:
                LOGERROR('could not uncompress %s' % sqlitecurve)

    except Exception as e:
        LOGEXCEPTION('could not uncompress %s' % sqlitecurve)



def gzip_sqlitecurve(sqlitecurve, force=False):
    '''This just compresses the sqlitecurve in gzip format.

    FIXME: this doesn't work with gzip < 1.6 or non-GNU gzip (probably).

    '''

    # -k to keep the input file just in case something explodes
    if force:
        cmd = 'gzip -k -f %s' % sqlitecurve
    else:
        cmd = 'gzip -k %s' % sqlitecurve

    try:

        outfile = '%s.gz' % sqlitecurve

        if os.path.exists(outfile) and not force:
            # get rid of the .sqlite file only
            os.remove(sqlitecurve)
            return outfile

        else:
            procout = subprocess.check_output(cmd, shell=True)

            # check if the output file was successfully created
            if os.path.exists(outfile):
                return outfile
            else:
                LOGERROR('could not compress %s' % sqlitecurve)
                return None

    except subprocess.CalledProcessError:
        LOGEXCEPTION('could not compress %s' % sqlitecurve)
        return None



def gunzip_sqlitecurve(sqlitecurve):
    '''This just uncompresses the sqlitecurve in gzip format.

    FIXME: this doesn't work with gzip < 1.6 or non-GNU gzip (probably).

    '''

    # -k to keep the input .gz just in case something explodes
    cmd = 'gunzip -k %s' % sqlitecurve

    try:
        procout = subprocess.check_output(cmd, shell=True)
        return sqlitecurve.replace('.gz','')
    except subprocess.CalledProcessError:
        LOGERROR('could not uncompress %s' % sqlitecurve)
        return None



###############################################
## DECIDE WHICH COMPRESSION FUNCTIONS TO USE ##
###############################################

try:
    GZIPTEST = subprocess.check_output(
        'gzip --version',
        shell=True
    ).decode().split('\n')[0].split()[-1]
    GZIPTEST = float(GZIPTEST)
    if GZIPTEST and GZIPTEST > 1.5:
        compress_sqlitecurve = gzip_sqlitecurve
        uncompress_sqlitecurve = gunzip_sqlitecurve
    else:
        LOGWARNING('gzip > 1.5 not available, using Python (gun)zip support')
        compress_sqlitecurve = pycompress_sqlitecurve
        uncompress_sqlitecurve = pyuncompress_sqlitecurve
except:
    compress_sqlitecurve = pycompress_sqlitecurve
    uncompress_sqlitecurve = pyuncompress_sqlitecurve
    GZIPTEST = None
    LOGWARNING('gzip > 1.5 not available, using Python (gun)zip support')



###################################
## READING SQLITECURVE FUNCTIONS ##
###################################

def validate_sqlitecurve_filters(filterstring, lccolumns):
    '''This validates the sqlitecurve filter string.

    This MUST be valid SQL but not contain any commands.

    '''

    # first, lowercase, then squeeze to single spaces
    stringelems = squeeze(filterstring).lower()

    # replace shady characters
    stringelems = filterstring.replace('(','')
    stringelems = stringelems.replace(')','')
    stringelems = stringelems.replace(',','')
    stringelems = stringelems.replace("'",'"')
    stringelems = stringelems.replace('\n',' ')
    stringelems = stringelems.replace('\t',' ')
    stringelems = squeeze(stringelems)

    # split into words
    stringelems = stringelems.split(' ')
    stringelems = [x.strip() for x in stringelems]

    # get rid of all numbers
    stringwords = []
    for x in stringelems:
        try:
            floatcheck = float(x)
        except ValueError as e:
            stringwords.append(x)

    # get rid of everything within quotes
    stringwords2 = []
    for x in stringwords:
        if not(x.startswith('"') and x.endswith('"')):
            stringwords2.append(x)
    stringwords2 = [x for x in stringwords2 if len(x) > 0]

    # check the filterstring words against the allowed words
    wordset = set(stringwords2)

    # generate the allowed word set for these LC columns
    allowedwords = SQLITE_ALLOWED_WORDS + lccolumns
    checkset = set(allowedwords)

    validatecheck = list(wordset - checkset)

    # if there are words left over, then this filter string is suspicious
    if len(validatecheck) > 0:

        # check if validatecheck contains an elem with % in it
        LOGWARNING("provided SQL filter string '%s' "
                   "contains non-allowed keywords" % filterstring)
        return None

    else:
        return filterstring



def read_and_filter_sqlitecurve(lcfile,
                                columns=None,
                                sqlfilters=None,
                                raiseonfail=False,
                                returnarrays=True,
                                forcerecompress=False):
    '''This reads the sqlitecurve and optionally filters it.

    Returns columns requested in columns. If None, then returns all columns
    present in the latest columnlist in the lightcurve.

    If sqlfilters is not None, it must be a list of text sql filters that apply
    to the columns in the lightcurve.

    This returns an lcdict with an added 'lcfiltersql' key that indicates what
    the parsed SQL filter string was.

    If forcerecompress = True, will recompress the un-gzipped sqlitecurve even
    if the gzipped form exists on disk already.

    '''

    # we're proceeding with reading the LC...
    try:

        # if this file is a gzipped sqlite3 db, then gunzip it
        if '.gz' in lcfile[-4:]:
            lcf = uncompress_sqlitecurve(lcfile)
        else:
            lcf = lcfile

        db = sql.connect(lcf)
        cur = db.cursor()

        # get the objectinfo from the sqlitecurve
        query = ("select * from objectinfo")
        cur.execute(query)
        objectinfo = cur.fetchone()

        # get the lcinfo from the sqlitecurve
        query = ("select * from lcinfo "
                 "order by version desc limit 1")
        cur.execute(query)
        lcinfo = cur.fetchone()

        (lcversion, lcdatarelease, lccols, lcsortcol,
         lcapertures, lcbestaperture,
         objinfocols, objidcol,
         lcunixtime, lcgitrev, lccomment) = lcinfo

        # load the JSON dicts
        lcapertures = json.loads(lcapertures)
        lcbestaperture = json.loads(lcbestaperture)

        objectinfokeys = objinfocols.split(',')
        objectinfodict = {x:y for (x,y) in zip(objectinfokeys, objectinfo)}
        objectid = objectinfodict[objidcol]

        # need to generate the objectinfo dict and the objectid from the lcinfo
        # columns

        # get the filters from the sqlitecurve
        query = ("select * from filters")
        cur.execute(query)
        filterinfo = cur.fetchall()

        # validate the requested columns
        if columns and all([x in lccols.split(',') for x in columns]):
            LOGINFO('retrieving columns %s' % columns)
            proceed = True
        elif columns is None:
            columns = lccols.split(',')
            proceed = True
        else:
            proceed = False

        # bail out if there's a problem and tell the user what happened
        if not proceed:
            # recompress the lightcurve at the end
            if '.gz' in lcfile[-4:] and lcf:
                dcf = compress_sqlitecurve(lcf, force=forcerecompress)
            LOGERROR('requested columns are invalid!')
            return None, "requested columns are invalid"

        # create the lcdict with the object, lc, and filter info
        lcdict = {'objectid':objectid,
                  'objectinfo':objectinfodict,
                  'objectinfokeys':objectinfokeys,
                  'lcversion':lcversion,
                  'datarelease':lcdatarelease,
                  'columns':columns,
                  'lcsortcol':lcsortcol,
                  'lcapertures':lcapertures,
                  'lcbestaperture':lcbestaperture,
                  'lastupdated':lcunixtime,
                  'lcserver':lcgitrev,
                  'comment':lccomment,
                  'filters':filterinfo}

        # validate the SQL filters for this LC
        if ((sqlfilters is not None) and
            (isinstance(sqlfilters,str) or isinstance(sqlfilters,unicode))):

            # give the validator the sqlfilters string and a list of lccols in
            # the lightcurve
            validatedfilters = validate_sqlitecurve_filters(sqlfilters,
                                                            lccols.split(','))
            if validatedfilters is not None:
                LOGINFO('filtering LC using: %s' % validatedfilters)
                filtersok = True
            else:
                filtersok = False
        else:
            validatedfilters = None
            filtersok = None

        # now read all the required columns in the order indicated

        # we use the validated SQL filter string here
        if validatedfilters is not None:

            query = ("select {columns} from lightcurve where {sqlfilter} "
                     "order by {sortcol} asc").format(
                         columns=','.join(columns), # columns is always a list
                         sqlfilter=validatedfilters,
                         sortcol=lcsortcol
                     )
            lcdict['lcfiltersql'] = validatedfilters

        else:
            query = ("select %s from lightcurve order by %s asc") % (
                ','.join(columns),
                lcsortcol
            )

        cur.execute(query)
        lightcurve = cur.fetchall()

        if lightcurve and len(lightcurve) > 0:

            lightcurve = list(zip(*lightcurve))
            lcdict.update({x:y for (x,y) in zip(lcdict['columns'],
                                                lightcurve)})
            lcok = True

            # update the ndet after filtering
            lcdict['objectinfo']['ndet'] = len(lightcurve[0])

        else:
            LOGWARNING('LC for %s has no detections' % lcdict['objectid'])

            # fill the lightcurve with empty lists to indicate that it is empty
            lcdict.update({x:y for (x,y) in
                           zip(lcdict['columns'],
                               [[] for x in lcdict['columns']])})
            lcok = False

        # generate the returned lcdict and status message
        if filtersok is True and lcok:
            statusmsg = 'SQL filters OK, LC OK'
        elif filtersok is None and lcok:
            statusmsg = 'no SQL filters, LC OK'
        elif filtersok is False and lcok:
            statusmsg = 'SQL filters invalid, LC OK'
        else:
            statusmsg = 'LC retrieval failed'

        returnval = (lcdict, statusmsg)

        # recompress the lightcurve at the end
        if '.gz' in lcfile[-4:] and lcf:
            dcf = compress_sqlitecurve(lcf, force=forcerecompress)


        # return ndarrays if that's set
        if returnarrays:
            for column in lcdict['columns']:
                lcdict[column] = np.array([x if x is not None else np.nan
                                           for x in lcdict[column]])

    except Exception as e:

        LOGEXCEPTION('could not open sqlitecurve %s' % lcfile)
        returnval = (None, 'error while reading lightcurve file')

        # recompress the lightcurve at the end
        if '.gz' in lcfile[-4:] and lcf:
            dcf = compress_sqlitecurve(lcf, force=forcerecompress)

        if raiseonfail:
            raise

    return returnval


############################
## DESCRIBING THE COLUMNS ##
############################

DESCTEMPLATE = '''\
OBJECT
------

objectid = {objectid}
hatid = {hatid}; twomassid = {twomassid}
network = {network}; stations = {stations}; ndet = {ndet}

ra = {ra}; decl = {decl}
pmra = {pmra}; pmra_err = {pmra_err}
pmdecl = {pmdecl}; pmdecl_err = {pmdecl_err}

jmag = {jmag}; hmag = {hmag}; kmag = {kmag}; bmag = {bmag}; vmag = {vmag}
sdssg = {sdssg}; sdssr = {sdssr}; sdssi = {sdssi}

METADATA
--------

datarelease = {datarelease}; lcversion = {lcversion}
lastupdated = {lastupdated:.3f}; lcserver = {lcserver}
comment = {comment}
lcbestaperture = {lcbestaperture}
lcsortcol = {lcsortcol}
lcfiltersql = {lcfiltersql}
lcnormcols = {lcnormcols}

CAMFILTERS
----------

{filterdefs}

PHOTAPERTURES
-------------

{aperturedefs}

LIGHT CURVE COLUMNS
-------------------

{columndefs}
'''


def describe(lcdict, returndesc=False):
    '''
    This describes the light curve object and columns present.

    '''

    # figure out the columndefs part of the header string
    columndefs = []

    for colind, column in enumerate(lcdict['columns']):

        if '_' in column:
            colkey, colap = column.split('_')
            coldesc = COLUMNDEFS[colkey][0] % colap
        else:
            coldesc = COLUMNDEFS[column][0]

        columndefstr = '%03i - %s - %s' % (colind,
                                             column,
                                             coldesc)
        columndefs.append(columndefstr)

    columndefs = '\n'.join(columndefs)

    # figure out the filterdefs
    filterdefs = []

    for row in lcdict['filters']:

        filterid, filtername, filterdesc = row
        filterdefstr = '%s - %s - %s' % (filterid,
                                           filtername,
                                           filterdesc)
        filterdefs.append(filterdefstr)

    filterdefs = '\n'.join(filterdefs)


    # figure out the apertures
    aperturedefs = []
    for key in sorted(lcdict['lcapertures'].keys()):
        aperturedefstr = '%s - %.2f px' % (key, lcdict['lcapertures'][key])
        aperturedefs.append(aperturedefstr)

    aperturedefs = '\n'.join(aperturedefs)

    # now fill in the description
    description = DESCTEMPLATE.format(
        objectid=lcdict['objectid'],
        hatid=lcdict['objectinfo']['hatid'],
        twomassid=lcdict['objectinfo']['twomassid'].strip(),
        ra=lcdict['objectinfo']['ra'],
        decl=lcdict['objectinfo']['decl'],
        pmra=lcdict['objectinfo']['pmra'],
        pmra_err=lcdict['objectinfo']['pmra_err'],
        pmdecl=lcdict['objectinfo']['pmdecl'],
        pmdecl_err=lcdict['objectinfo']['pmdecl_err'],
        jmag=lcdict['objectinfo']['jmag'],
        hmag=lcdict['objectinfo']['hmag'],
        kmag=lcdict['objectinfo']['kmag'],
        bmag=lcdict['objectinfo']['bmag'],
        vmag=lcdict['objectinfo']['vmag'],
        sdssg=lcdict['objectinfo']['sdssg'],
        sdssr=lcdict['objectinfo']['sdssr'],
        sdssi=lcdict['objectinfo']['sdssi'],
        ndet=lcdict['objectinfo']['ndet'],
        lcsortcol=lcdict['lcsortcol'],
        lcbestaperture=json.dumps(lcdict['lcbestaperture'],ensure_ascii=True),
        network=lcdict['objectinfo']['network'],
        stations=lcdict['objectinfo']['stations'],
        lastupdated=lcdict['lastupdated'],
        datarelease=lcdict['datarelease'],
        lcversion=lcdict['lcversion'],
        lcserver=lcdict['lcserver'],
        comment=lcdict['comment'],
        lcfiltersql=(lcdict['lcfiltersql'] if 'lcfiltersql' in lcdict else ''),
        lcnormcols=(lcdict['lcnormcols'] if 'lcnormcols' in lcdict else ''),
        filterdefs=filterdefs,
        columndefs=columndefs,
        aperturedefs=aperturedefs
        )

    print(description)

    if returndesc:
        return description



#############################
## READING CSV LIGHTCURVES ##
#############################

def smartcast(castee, caster, subval=None):
    '''
    This just tries to apply the caster function to castee.

    Returns None on failure.

    '''

    try:
        return caster(castee)
    except Exception as e:
        if caster is float or caster is int:
            return nan
        elif caster is str:
            return ''
        else:
            return subval



# these are the keys used in the metadata section of the CSV LC
METAKEYS = {'objectid':str,
            'hatid':str,
            'twomassid':str,
            'ucac4id':str,
            'network':str,
            'stations':str,
            'ndet':int,
            'ra':float,
            'decl':float,
            'pmra':float,
            'pmra_err':float,
            'pmdecl':float,
            'pmdecl_err':float,
            'jmag':float,
            'hmag':float,
            'kmag':float,
            'bmag':float,
            'vmag':float,
            'sdssg':float,
            'sdssr':float,
            'sdssi':float}



def parse_csv_header(header):
    '''
    This parses the CSV header from the CSV HAT sqlitecurve.

    Returns a dict that can be used to update an existing lcdict with the
    relevant metadata info needed to form a full LC.

    '''

    # first, break into lines
    headerlines = header.split('\n')
    headerlines = [x.lstrip('# ') for x in headerlines]

    # next, find the indices of the metadata sections
    objectstart = headerlines.index('OBJECT')
    metadatastart = headerlines.index('METADATA')
    camfilterstart = headerlines.index('CAMFILTERS')
    photaperturestart = headerlines.index('PHOTAPERTURES')
    columnstart = headerlines.index('COLUMNS')
    lcstart = headerlines.index('LIGHTCURVE')

    # get the lines for the header sections
    objectinfo = headerlines[objectstart+1:metadatastart-1]
    metadatainfo = headerlines[metadatastart+1:camfilterstart-1]
    camfilterinfo = headerlines[camfilterstart+1:photaperturestart-1]
    photapertureinfo = headerlines[photaperturestart+1:columnstart-1]
    columninfo = headerlines[columnstart+1:lcstart-1]

    # parse the header sections and insert the appropriate key-val pairs into
    # the lcdict
    metadict = {'objectinfo':{}}

    # first, the objectinfo section
    objectinfo = [x.split(';') for x in objectinfo]

    for elem in objectinfo:
        for kvelem in elem:
            key, val = kvelem.split(' = ',1)
            metadict['objectinfo'][key.strip()] = (
                smartcast(val, METAKEYS[key.strip()])
                )

    # the objectid belongs at the top level
    metadict['objectid'] = metadict['objectinfo']['objectid'][:]
    del metadict['objectinfo']['objectid']

    # get the lightcurve metadata
    metadatainfo = [x.split(';') for x in metadatainfo]
    for elem in metadatainfo:
        for kvelem in elem:

            try:
                key, val = kvelem.split(' = ',1)

                # get the lcbestaperture into a dict again
                if key.strip() == 'lcbestaperture':
                    val = json.loads(val)

                # get the lcversion and datarelease as integers
                if key.strip() in ('datarelease', 'lcversion'):
                    val = int(val)

                # get the lastupdated as a float
                if key.strip() == 'lastupdated':
                    val = float(val)

                # put the key-val into the dict
                metadict[key.strip()] = val

            except Exception as e:

                LOGWARNING('could not understand header element "%s",'
                           ' skipped.' % kvelem)


    # get the camera filters
    metadict['filters'] = []
    for row in camfilterinfo:
        filterid, filtername, filterdesc = row.split(' - ')
        metadict['filters'].append((int(filterid),
                                    filtername,
                                    filterdesc))

    # get the photometric apertures
    metadict['lcapertures'] = {}
    for row in photapertureinfo:
        apnum, appix = row.split(' - ')
        appix = float(appix.rstrip(' px'))
        metadict['lcapertures'][apnum.strip()] = appix

    # get the columns
    metadict['columns'] = []

    for row in columninfo:
        colnum, colname, coldesc = row.split(' - ')
        metadict['columns'].append(colname)

    return metadict



def read_csvlc(lcfile):
    '''
    This reads the HAT data server producd CSV light curve into a lcdict.

    lcfile is the HAT gzipped CSV LC (with a .hatlc.csv.gz extension)

    '''

    # read in the file and split by lines
    if '.gz' in os.path.basename(lcfile):
        LOGINFO('reading gzipped HATLC: %s' % lcfile)
        infd = gzip.open(lcfile,'rb')
    else:
        LOGINFO('reading HATLC: %s' % lcfile)
        infd = open(lcfile,'rb')

    lctext = infd.read().decode() # argh Python 3
    infd.close()

    # figure out the header and get the LC columns
    lcstart = lctext.index('# LIGHTCURVE\n')
    lcheader = lctext[:lcstart+12]
    lccolumns = lctext[lcstart+13:].split('\n')
    lccolumns = [x for x in lccolumns if len(x) > 0]

    # initialize the lcdict and parse the CSV header
    lcdict = parse_csv_header(lcheader)

    # tranpose the LC rows into columns
    lccolumns = [x.split(',') for x in lccolumns]
    lccolumns = list(zip(*lccolumns)) # argh more Python 3

    # write the columns to the dict
    for colind, col in enumerate(lcdict['columns']):

        if (col.split('_')[0] in LC_MAG_COLUMNS or
            col.split('_')[0] in LC_ERR_COLUMNS or
            col.split('_')[0] in LC_FLAG_COLUMNS):
            lcdict[col] = np.array([smartcast(x,
                                              COLUMNDEFS[col.split('_')[0]][2])
                                    for x in lccolumns[colind]])

        elif col in COLUMNDEFS:
            lcdict[col] = np.array([smartcast(x,COLUMNDEFS[col][2])
                                    for x in lccolumns[colind]])

        else:
            LOGWARNING('lcdict col %s has no formatter available' % col)
            continue

    return lcdict



##########################
## NORMALIZING  LCDICTS ##
##########################

def find_lc_timegroups(lctimes, mingap=4.0):
    '''
    This finds the gaps in the lightcurve, so we can figure out which times are
    for consecutive observations and which represent gaps between
    seasons.

    lctimes is assumed to be in some form of JD.

    min_gap defines how much the difference between consecutive measurements is
    allowed to be to consider them as parts of different timegroups. By default
    it is set to 4.0 days.

    Returns number of groups and Python slice objects for each group like so:

    (ngroups, [slice(start_ind_1, end_ind_1), ...])

    '''

    lc_time_diffs = [(lctimes[x] - lctimes[x-1]) for x in range(1,len(lctimes))]
    lc_time_diffs = np.array(lc_time_diffs)

    group_start_indices = np.where(lc_time_diffs > mingap)[0]

    if len(group_start_indices) > 0:

        group_indices = []

        for i, gindex in enumerate(group_start_indices):

            if i == 0:
                group_indices.append(slice(0,gindex+1))
            else:
                group_indices.append(slice(group_start_indices[i-1]+1,gindex+1))


        # at the end, add the slice for the last group to the end of the times
        # array
        group_indices.append(slice(group_start_indices[-1]+1,len(lctimes)))

    # if there's no large gap in the LC, then there's only one group to worry
    # about
    else:
        group_indices = [slice(0,len(lctimes))]


    return len(group_indices), group_indices



def normalize_lcdict(lcdict,
                     timecol='rjd',
                     magcols='all',
                     mingap=4.0,
                     normto='sdssr',
                     debugmode=False):
    '''This normalizes magcols in lcdict using timecol to find timegroups.

    Returns the lcdict with an added lcnormcols key indicating which columns
    were normalized.

    the lcnormcols key:val is like so:

    'lcnormcols': '{normalized magcols repr} - {mingap} - {globalmedianflag}'

    the normto kwarg is one of the following strings:

    'globalmedian' -> norms each mag to the global median of the LC column
    'zero'         -> norms each mag to zero

    or one of:

    'jmag', 'hmag', 'kmag', 'bmag', 'vmag', 'sdssg', 'sdssr', 'sdssi'

    each mag for each column will be normalized to the value of these keys in
    the lcdict

    '''

    # check if this lc has been normalized already. return as-is if so
    if 'lcnormcols' in lcdict and len(lcdict['lcnormcols']) > 0:
        LOGWARNING('this lightcurve is already normalized, returning...')
        return lcdict

    # first, get the LC timegroups
    if timecol in lcdict:
        times = lcdict[timecol]
    elif 'rjd' in lcdict:
        times = lcdict['rjd']
    # if there aren't any time columns in this lcdict, then we can't do any
    # normalization, return it as-is
    else:
        LOGERROR("can't figure out the time column to use, lcdict cols = %s" %
                 lcdict['columns'])
        return lcdict

    ngroups, timegroups = find_lc_timegroups(np.array(times),
                                             mingap=mingap)

    apertures = sorted(lcdict['lcapertures'].keys())

    aimcols = [('aim_%s' % x) for x in apertures if ('aim_%s' % x) in lcdict]
    armcols = [('arm_%s' % x) for x in apertures if ('arm_%s' % x) in lcdict]
    aepcols = [('aep_%s' % x)for x in apertures if ('aep_%s' % x) in lcdict]
    atfcols = [('atf_%s' % x) for x in apertures if ('atf_%s' % x) in lcdict]
    psimcols = [x for x in ['psim','psrm','psep','pstf'] if x in lcdict]
    irmcols = [('irm_%s' % x) for x in apertures if ('irm_%s' % x) in lcdict]
    iepcols = [('iep_%s' % x) for x in apertures if ('iep_%s' % x) in lcdict]
    itfcols = [('itf_%s' % x) for x in apertures if ('itf_%s' % x) in lcdict]

    # next, find all the mag columns to normalize
    if magcols == 'all':
        cols_to_normalize = (aimcols + irmcols + aepcols + atfcols +
                             psimcols + irmcols + iepcols + itfcols)
    elif magcols == 'redmags':
        cols_to_normalize = (irmcols + (['psrm'] if 'psrm' in lcdict else []) +
                             irmcols)
    elif magcols == 'epdmags':
        cols_to_normalize = (aepcols + (['psep'] if 'psep' in lcdict else []) +
                             iepcols)
    elif magcols == 'tfamags':
        cols_to_normalize = (atfcols + (['pstf'] if 'pstf' in lcdict else []) +
                             itfcols)
    elif magcols == 'epdtfa':
        cols_to_normalize = (aepcols + (['psep'] if 'psep' in lcdict else []) +
                             iepcols + atfcols +
                             (['pstf'] if 'pstf' in lcdict else []) +
                             itfcols)
    else:
        cols_to_normalize = magcols.split(',')
        cols_to_normalize = [x.strip() for x in cols_to_normalize]

    colsnormalized = []

    # now, normalize each column
    for col in cols_to_normalize:

        if col in lcdict:

            mags = lcdict[col]
            mags = [(nan if x is None else x) for x in mags]
            mags = np.array(mags)

            colsnormalized.append(col)

            # find all the non-nan indices
            finite_ind = np.isfinite(mags)

            if any(finite_ind):

                # find the global median
                global_mag_median = np.median(mags[finite_ind])

                # go through the groups and normalize them to the median for
                # each group
                for tgind, tg in enumerate(timegroups):

                    finite_ind = np.isfinite(mags[tg])

                    # find this timegroup's median mag and normalize the mags in
                    # it to this median
                    group_median = np.median((mags[tg])[finite_ind])
                    mags[tg] = mags[tg] - group_median

                    if debugmode:
                        LOGDEBUG('%s group %s: elems %s, '
                                 'finite elems %s, median mag %s' %
                                 (col, tgind,
                                  len(mags[tg]),
                                  len(finite_ind),
                                  group_median))

            else:
                LOGWARNING('column %s is all nan, skipping...' % col)
                continue


            # now that everything is normalized to 0.0, add the global median
            # offset back to all the mags and write the result back to the dict
            if normto == 'globalmedian':
                mags = mags + global_mag_median
            elif normto in ('jmag', 'hmag', 'kmag',
                            'bmag', 'vmag',
                            'sdssg', 'sdssr', 'sdssi'):

                if (normto in lcdict['objectinfo'] and
                  lcdict['objectinfo'][normto] is not None):
                    mags = mags + lcdict['objectinfo'][normto]

                else:
                    LOGWARNING('no %s available in lcdict, '
                               'normalizing to global mag median' % normto)
                    normto = 'globalmedian'
                    mags = mags + global_mag_median

            lcdict[col] = mags

        else:

            LOGWARNING('column %s is not present, skipping...' % col)
            continue

    # add the lcnormcols key to the lcdict
    lcnormcols = ('cols normalized: %s - '
                  'min day gap: %s - '
                  'normalized to: %s') % (
        repr(colsnormalized),
        mingap,
        normto
    )
    lcdict['lcnormcols'] = lcnormcols

    return lcdict



def normalize_lcdict_byinst(
        lcdict,
        magcols='all',
        normto='sdssr',
        debugmode=False
):
    '''This is a function to normalize light curves across all instrument
    combinations present.

    Use this to normalize a light curve containing a variety of:

    - HAT station IDs
    - filters
    - observed field names
    - HAT project IDs
    - exposure times

    See the docstring for normalize_lcdict for more about the magcols and normto
    kwargs. EXCEPTION: normalize_lcdict_instruments does not respect
    normto='globalmedian'.

    '''

    # check if this lc has been normalized already. return as-is if so
    if 'lcinstnormcols' in lcdict and len(lcdict['lcinstnormcols']) > 0:
        LOGWARNING('this lightcurve is already normalized by instrument keys'
                   ', returning...')
        return lcdict

    # generate the normalization key
    allkeys = ['%s-%s-%s-%s-%s' % (x,y,z,u,v) for (x,y,z,u,v)
               in zip(lcdict['stf'],
                      lcdict['flt'],
                      lcdict['fld'],
                      lcdict['prj'],
                      lcdict['exp'])]
    allkeys = np.array(allkeys)

    normkeys = np.unique(allkeys)

    # figure out the apertures
    apertures = sorted(lcdict['lcapertures'].keys())

    # put together the column names
    aimcols = [('aim_%s' % x) for x in apertures if ('aim_%s' % x) in lcdict]
    armcols = [('arm_%s' % x) for x in apertures if ('arm_%s' % x) in lcdict]
    aepcols = [('aep_%s' % x)for x in apertures if ('aep_%s' % x) in lcdict]
    atfcols = [('atf_%s' % x) for x in apertures if ('atf_%s' % x) in lcdict]
    psimcols = [x for x in ['psim','psrm','psep','pstf'] if x in lcdict]
    irmcols = [('irm_%s' % x) for x in apertures if ('irm_%s' % x) in lcdict]
    iepcols = [('iep_%s' % x) for x in apertures if ('iep_%s' % x) in lcdict]
    itfcols = [('itf_%s' % x) for x in apertures if ('itf_%s' % x) in lcdict]

    # next, find all the mag columns to normalize
    if magcols == 'all':
        cols_to_normalize = (aimcols + irmcols + aepcols + atfcols +
                             psimcols + irmcols + iepcols + itfcols)
    elif magcols == 'redmags':
        cols_to_normalize = (irmcols + (['psrm'] if 'psrm' in lcdict else []) +
                             irmcols)
    elif magcols == 'epdmags':
        cols_to_normalize = (aepcols + (['psep'] if 'psep' in lcdict else []) +
                             iepcols)
    elif magcols == 'tfamags':
        cols_to_normalize = (atfcols + (['pstf'] if 'pstf' in lcdict else []) +
                             itfcols)
    elif magcols == 'epdtfa':
        cols_to_normalize = (aepcols + (['psep'] if 'psep' in lcdict else []) +
                             iepcols + atfcols +
                             (['pstf'] if 'pstf' in lcdict else []) +
                             itfcols)
    else:
        cols_to_normalize = magcols.split(',')
        cols_to_normalize = [x.strip() for x in cols_to_normalize]

    colsnormalized = []

    # go through each column and normalize them
    for col in cols_to_normalize:

        if col in lcdict:

            # note: this requires the columns in ndarray format
            # unlike normalize_lcdict
            thismags = lcdict[col]

            # go through each key in normusing
            for nkey in normkeys:

                thisind = allkeys == nkey

                # make sure we have at least 3 elements in the matched set of
                # magnitudes corresponding to this key. also make sure that the
                # magnitudes corresponding to this key aren't all nan.
                thismagsize = thismags[thisind].size
                thismagfinite = np.where(np.isfinite(thismags[thisind]))[0].size

                if thismagsize > 2 and thismagfinite > 2:

                    # do the normalization and update the thismags in the lcdict
                    medmag = np.nanmedian(thismags[thisind])
                    lcdict[col][thisind] = lcdict[col][thisind] - medmag

                    if debugmode:
                        LOGINFO('currkey %s, nelem %s, '
                                'medmag %s' %
                                (nkey, len(thismags[thisind]), medmag))

                # we remove mags that correspond to keys with less than 3
                # (finite) elements because we can't get the median mag
                # correctly and renormalizing them to zero would just set them
                # to zero
                else:

                    lcdict[col][thisind] = np.nan

            # everything should now be normalized to zero
            # add back the requested normto
            if normto in ('jmag', 'hmag', 'kmag',
                          'bmag', 'vmag',
                          'sdssg', 'sdssr', 'sdssi'):

                if (normto in lcdict['objectinfo'] and
                    lcdict['objectinfo'][normto] is not None):
                    lcdict[col] = lcdict[col] + lcdict['objectinfo'][normto]

                else:
                    LOGWARNING('no %s available in lcdict, '
                               'normalizing to 0.0' % normto)
                    normto = 'zero'

            # update the colsnormalized list
            colsnormalized.append(col)

        else:

            LOGWARNING('column %s is not present, skipping...' % col)
            continue

    # add the lcnormcols key to the lcdict
    lcinstnormcols = ('cols normalized: %s - '
                      'normalized to: %s') % (
                          repr(colsnormalized),
                          normto
                      )
    lcdict['lcinstnormcols'] = lcinstnormcols

    return lcdict
